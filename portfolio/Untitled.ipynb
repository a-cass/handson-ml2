{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression (without Scikit-Learn)\n",
    "This notebook provides a solution to Chapter 4 Exercise 12:\n",
    ">*Implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn)*\n",
    "\n",
    "For this task I will use the [IRIS](https://archive.ics.uci.edu/ml/datasets/iris) that is packaged with Scikit-Learn and accessible via the `datasets` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['data'].shape # 150 samples with 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to allow me to compare my results to those in the corresponding chapter notebook, I will focus on the `petal length (cm)` and `petal_width (cm)` features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = ['petal length (cm)', 'petal width (cm)']\n",
    "feats_idx = [iris['feature_names'].index(ft) for ft in feats]\n",
    "feats_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data'][:,feats_idx]\n",
    "y = iris['target']\n",
    "\n",
    "# Need to add x0 = 1 for the bias term - note that the Logistic Regression\n",
    "# models in Scikit-Learn will automatically add this by default\n",
    "X_b = np.c_[np.ones((X.shape[0],1)),X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (150,))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m  = number of samples\n",
    "# n = number of features(use X as we dont want to count the bias term)\n",
    "m, n = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 90\n",
      "Number of validation samples: 30\n",
      "Number of test samples: 30\n",
      "(90, 3)\n",
      "(90,)\n",
      "(30, 3)\n",
      "(30,)\n",
      "(30, 3)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "ratio_valid = 0.2\n",
    "ratio_test = 0.2\n",
    "\n",
    "valid_count = int(m * ratio_valid)\n",
    "test_count = int(m * ratio_test)\n",
    "train_count = m - valid_count - test_count\n",
    "\n",
    "print(f'Number of training samples: {train_count}')\n",
    "print(f'Number of validation samples: {valid_count}')\n",
    "print(f'Number of test samples: {test_count}')\n",
    "\n",
    "# set random seed to same as exercise solution\n",
    "np.random.seed(2042)\n",
    "perms = np.random.permutation(m)\n",
    "\n",
    "X_train = X_b[perms[:train_count]]\n",
    "y_train = y[perms[:train_count]]\n",
    "\n",
    "X_valid = X_b[perms[train_count:train_count + valid_count]]\n",
    "y_valid = y[perms[train_count:train_count + valid_count]]\n",
    "\n",
    "X_test = X_b[perms[train_count + valid_count:]]\n",
    "y_test = y[perms[train_count + valid_count:]]\n",
    "\n",
    "print(X_train.shape, y_train.shape,\n",
    "      X_valid.shape, y_valid.shape,\n",
    "      X_test.shape, y_test.shape,\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For multiclass classifications we need to convert the single target\n",
    "# class integer into an array of values indictating whether or not\n",
    "# the sample belongs to each class - 0/1.\n",
    "# This is similar to One Hot Encoding!\n",
    "\n",
    "def class_probabilities(y):\n",
    "    n_classes = y.max() + 1\n",
    "    m = len(y)\n",
    "    Y_one_hot = np.zeros((m, n_classes))\n",
    "    \n",
    "    # indexes are determined pairwise i.e. [sample_1,target_1] = 1\n",
    "    # therefore this has the effect of, for each sample, setting\n",
    "    # a value of 1 in the one hot column where the index = target class y\n",
    "    # e.g.\n",
    "    # sample_1 = row 0, target_1 = y[0] = 2\n",
    "    # Y_one_hot[0,2] = 1\n",
    "    \n",
    "    Y_one_hot[np.arange(m), y] = 1\n",
    "    return Y_one_hot\n",
    "\n",
    "class_probabilities(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
